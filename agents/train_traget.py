# train_dqn.py å®Œæ•´ä»£ç¢¼ - ä½¿ç”¨ Target Network æ”¹é€² DQN (å®Œæ•´è¨»è§£ç‰ˆ)

import pygame
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import yaml
from collections import deque
from envs.baby_pong_env import BabyPongBounceEnv as BabyPongEnv
import matplotlib.pyplot as plt
import time
from torch.utils.tensorboard import SummaryWriter

# å®šç¾©Qå­¸ç¿’ç¥ç¶“ç¶²è·¯æ¨¡å‹
class QNet(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(QNet, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )

    def forward(self, x):
        return self.fc(x)

# å¾è¨­å®šæª”ä¸­è®€å–é…ç½®
with open("config.yaml", "r") as f:
    cfg = yaml.safe_load(f)

# åˆå§‹åŒ–ç’°å¢ƒ
env = BabyPongEnv(**cfg['env'])

# åˆå§‹åŒ–æ¨¡å‹åŠç›®æ¨™æ¨¡å‹
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
model = QNet(state_dim, action_dim)
target_model = QNet(state_dim, action_dim)
target_model.load_state_dict(model.state_dict())  # åˆå§‹åŒ–ç›®æ¨™æ¨¡å‹æ¬Šé‡èˆ‡ä¸»æ¨¡å‹ä¸€è‡´
target_model.eval()  # ç›®æ¨™æ¨¡å‹è¨­ç‚ºè©•ä¼°æ¨¡å¼ï¼Œä¸é€²è¡Œæ¢¯åº¦æ›´æ–°

optimizer = optim.Adam(model.parameters(), lr=cfg['train']['lr'])
memory = deque(maxlen=cfg['train']['memory_size'])  # ç¶“é©—å›æ”¾è¨˜æ†¶åº«
epsilon = 1.0  # Îµ-greedyæ¢ç´¢åˆå§‹å€¼
rewards = []
start_episode = 0
writer = SummaryWriter(log_dir="runs/baby_pong")
target_update_freq = cfg['train'].get('target_update_freq', 10)  # ç›®æ¨™ç¶²è·¯æ›´æ–°é »ç‡

# è¼‰å…¥å…ˆå‰è¨“ç·´éçš„æ¨¡å‹ï¼ˆè‹¥æœ‰ï¼‰
if cfg['train']['load_model'] and os.path.exists(cfg['train']['checkpoint_path']):
    checkpoint = torch.load(cfg['train']['checkpoint_path'])
    model.load_state_dict(checkpoint['model'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    epsilon = checkpoint['epsilon']
    start_episode = checkpoint['episode'] + 1
    target_model.load_state_dict(model.state_dict())
    print(f"âœ” Loaded checkpoint from episode {checkpoint['episode']}")
else:
    print("ğŸ†• Starting from scratch")

# è¨“ç·´å‡½æ•¸ï¼Œä½¿ç”¨batchæ›´æ–°æ¨¡å‹

def train():
    # ç¢ºèªè¨˜æ†¶åº«ä¸­æœ‰è¶³å¤ æ¨£æœ¬
    if len(memory) < cfg['train']['batch_size']:
        return

    # å¾è¨˜æ†¶åº«ä¸­éš¨æ©ŸæŠ½å–batch
    batch = random.sample(memory, cfg['train']['batch_size'])
    states, actions, rewards_, next_states, dones = zip(*batch)

    # è½‰æ›æˆtensor
    states = torch.tensor(np.array(states), dtype=torch.float32)
    next_states = torch.tensor(np.array(next_states), dtype=torch.float32)
    actions = torch.tensor(actions, dtype=torch.int64)
    rewards_ = torch.tensor(rewards_, dtype=torch.float32)
    dones = torch.tensor(dones, dtype=torch.bool)

    # è¨ˆç®—ç•¶å‰ç‹€æ…‹çš„Qå€¼
    q_values = model(states)
    # ä½¿ç”¨ç›®æ¨™æ¨¡å‹è¨ˆç®—ä¸‹ä¸€ç‹€æ…‹çš„Qå€¼ï¼ˆå›ºå®šç›®æ¨™ï¼‰
    with torch.no_grad():
        next_q_values = target_model(next_states)
        next_q_value = next_q_values.max(1)[0]

    # ä¾ç…§å¯¦éš›æ¡å–çš„è¡Œå‹•å–å¾—é æ¸¬Qå€¼
    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
    # è¨ˆç®—ç›®æ¨™Qå€¼ï¼ˆåŸºæ–¼Bellmanæ–¹ç¨‹ï¼‰
    expected_q = rewards_ + cfg['train']['gamma'] * next_q_value * (~dones)

    # è¨ˆç®—æå¤±å‡½æ•¸
    loss = nn.MSELoss()(q_value, expected_q)
    writer.add_scalar("Loss", loss.item(), len(rewards))

    # æ›´æ–°æ¨¡å‹åƒæ•¸
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# ä¸»è¨“ç·´è¿´åœˆ
for episode in range(start_episode, start_episode + cfg['train']['episodes']):
    state, _ = env.reset()
    total_reward = 0
    done = False

    while not done:
        # Îµ-greedyç­–ç•¥
        if random.random() < epsilon:
            action = env.action_space.sample()
        else:
            with torch.no_grad():
                action = model(torch.tensor(state, dtype=torch.float32)).argmax().item()

        # åŸ·è¡Œå‹•ä½œï¼Œç²å–ä¸‹ä¸€ç‹€æ…‹å’Œçå‹µ
        next_state, reward, done, _, _ = env.step(action)
        memory.append((state, action, reward, next_state, done))
        train()
        state = next_state
        total_reward += reward

    rewards.append(total_reward)
    writer.add_scalar("Reward", total_reward, episode)
    writer.add_scalar("Epsilon", epsilon, episode)
    epsilon = max(cfg['train']['min_epsilon'], epsilon * cfg['train']['epsilon_decay'])

    # å®šæœŸæ›´æ–°ç›®æ¨™ç¶²è·¯
    if (episode + 1) % target_update_freq == 0:
        target_model.load_state_dict(model.state_dict())
        print(f"ğŸ”„ Updated target network at episode {episode}")

    # å®šæœŸå„²å­˜æ¨¡å‹
    if (episode + 1) % cfg['train']['save_every'] == 0:
        model_path = f"checkpoints/model_traget_ep{episode}.pth"
        torch.save({
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'epsilon': epsilon,
            'episode': episode
        }, model_path)
        print(f"ğŸ’¾ Saved model to {model_path}")

    print(f"Episode {episode}: Total reward = {total_reward}")

    # å®šæœŸæ¸²æŸ“ç’°å¢ƒè§€å¯Ÿè¨“ç·´ç‹€æ…‹
    if (episode + 1) % cfg['train']['render_every'] == 0:
        state, _ = env.reset()
        done = False
        while not done:
            env.render()
            time.sleep(0.05)
            with torch.no_grad():
                action = model(torch.tensor(state, dtype=torch.float32)).argmax().item()
            state, _, done, _, _ = env.step(action)

# çµæŸç’°å¢ƒï¼Œé—œé–‰è³‡æº
env.close()
